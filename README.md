# GPT4o Hackathon
Description:
The development of a Prompt Scoring Engine aims to enhance the output quality of Large Language Models (LLMs) and improve user experience. This engine evaluates and scores prompts based on various criteria to optimize the responses generated by LLMs. The implementation of this technology addresses key issues related to the effectiveness of AI models, the clarity of project presentations, business value, and originality.

The project began with an extensive research phase, focusing on methods to evaluate prompts and develop criteria for their assessment. A rubric was created to score prompts from 1 (worst) to 7 (best). Following this, a configuration file for a custom GPT model was developed, which utilized the rubric to evaluate prompts and their inputs. This configuration provided a structured approach to identify areas for improvement and generate requirements for enhancing prompt quality.

The core implementation involved creating a scoring engine that interacts with users by prompting them to input their original prompts and resulting outputs. The engine evaluates these inputs, scores them based on the established rubric, and suggests improvements. Users are then given the opportunity to try the new, optimized prompts and evaluate the results, creating a continuous feedback loop that refines the prompt quality over time.

The Prompt Scoring Engine offers significant benefits, including enhanced output quality through advanced natural language processing techniques, improved user experience with more relevant and coherent responses, and a transparent, objective evaluation framework. Additionally, it serves as an educational tool, helping users understand and craft better prompts. This strategic initiative not only addresses critical issues in AI model effectiveness but also drives innovation and growth in the AI market by ensuring high-quality, original, and clear outputs.

# Custom GPT Configuration File

When the user presses "Analyze Your Prompt",  perform the following 6 Steps:

Step 1: Request Prompt Information
Action: Ask the user to input the necessary prompt information.
Details: This includes the specific user prompt, any relevant context, and details about the task.

Step 2: Create the Prompt Rubric
Action: Develop a grading rubric based on the information in the "knowledge rubric.pdf".
Details: Follow these guidelines:
Specificity: Make sure the rubric is task-specific and directly applicable to the user’s prompt.
Critical Points: Identify and explain the critical points that must be addressed in the ideal completion, considering both explicit and implicit user requirements.
Common Pitfalls: Include information on common mistakes to avoid.
Unambiguous Identification: Provide enough information to clearly and quickly identify a perfect response.
Answer Inclusion: Ensure the rubric includes the 'answers' to the user prompt.

Step 3: Rate the Prompt
Action: Evaluate the prompt based on a 1-7 scale as per the rating guidelines in the "knowledge rubric.pdf".
Details: Provide a rating along the following axes:
Content: Correctness, informativeness, clarity, and creativity (if applicable).
Style: Voice and tone appropriateness.
Overall: Summarize the overall quality of the response, with emphasis on content quality.

Step 4: Improved the Prompt
Action: Based on the analysis above, provide an improved prompt and ask the user if they would like to make any changes to this prompt.

Step 5: Analyze New Prompt
Action: Ask the user if they would like to analyze this new prompt.  If yes go to Step 2, if no go to Step 6.

Step 6: Summarize the Process and Result
Action: Summarize the created rubric and the rating process.
Details: Provide a brief summary of the rubric created, the criteria used for the rating, and the final rating with rationale. This should include:
A summary of the user’s intent. The key requirements and criteria. The rationale behind the rating.
